{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D0vEcPxSJ8hI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
      "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to c:\\users\\jason\\appdata\\local\\temp\\pip-req-build-eel5o1w6\n",
      "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 9e8c38aa5b92bbf0e20f65fc611fd43b43196859\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
      "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to c:\\users\\jason\\appdata\\local\\temp\\pip-install-r0ekff0k\\elegantrl_1f58174a581b4034a6a9b83445b87222\n",
      "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 37aac1f592e1add9f9fd37ae8db1094656009b76\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: alpaca-py<0.38,>=0.37 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from finrl==0.3.8) (0.37.0)\n",
      "Requirement already satisfied: alpaca-trade-api<4,>=3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from finrl==0.3.8) (3.2.0)\n",
      "Requirement already satisfied: ccxt<4,>=3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from finrl==0.3.8) (3.1.60)\n",
      "Requirement already satisfied: jqdatasdk<2,>=1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from finrl==0.3.8) (1.9.7)\n",
      "Requirement already satisfied: pandas-market-calendars<6,>=5 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from finrl==0.3.8) (5.1.1)\n",
      "Requirement already satisfied: pyfolio-reloaded<0.10,>=0.9 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from finrl==0.3.8) (0.9.9)\n",
      "Requirement already satisfied: pyportfolioopt<2,>=1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from finrl==0.3.8) (1.5.6)\n",
      "Requirement already satisfied: ray<3,>=2 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.49.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from finrl==0.3.8) (1.7.2)\n",
      "Requirement already satisfied: selenium<5,>=4 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from finrl==0.3.8) (4.32.0)\n",
      "Requirement already satisfied: stable-baselines3>=2.0.0a5 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.7.0)\n",
      "Requirement already satisfied: stockstats<0.6,>=0.5 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from finrl==0.3.8) (0.5.4)\n",
      "Requirement already satisfied: webdriver-manager<5,>=4 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from finrl==0.3.8) (4.0.2)\n",
      "Requirement already satisfied: wrds<4,>=3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from finrl==0.3.8) (3.4.0)\n",
      "Requirement already satisfied: yfinance<0.3,>=0.2 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from finrl==0.3.8) (0.2.58)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.0.3)\n",
      "Requirement already satisfied: pandas>=1.5.3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.11.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.30.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.5)\n",
      "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.8.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (10.4)\n",
      "Requirement already satisfied: numpy>=1.11.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.2.6)\n",
      "Requirement already satisfied: urllib3<2,>1.24 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.20)\n",
      "Requirement already satisfied: websocket-client<2,>=0.56.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.12.15)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.1)\n",
      "Requirement already satisfied: deprecation==2.1.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jason\\miniconda3\\lib\\site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.1)\n",
      "Requirement already satisfied: setuptools>=60.9.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ccxt<4,>=3->finrl==0.3.8) (72.1.0)\n",
      "Requirement already satisfied: certifi>=2018.1.18 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.8.3)\n",
      "Requirement already satisfied: cryptography>=2.6.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ccxt<4,>=3->finrl==0.3.8) (42.0.5)\n",
      "Requirement already satisfied: aiodns>=1.1.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ccxt<4,>=3->finrl==0.3.8) (3.5.0)\n",
      "Requirement already satisfied: yarl>=1.7.2 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ccxt<4,>=3->finrl==0.3.8) (1.20.1)\n",
      "Requirement already satisfied: six in c:\\users\\jason\\miniconda3\\lib\\site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.16.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.2.8 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.43)\n",
      "Requirement already satisfied: pymysql>=0.7.6 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.1.2)\n",
      "Requirement already satisfied: thriftpy2<=0.4.20,>=0.3.9 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (0.4.20)\n",
      "Requirement already satisfied: tzdata in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pandas-market-calendars<6,>=5->finrl==0.3.8) (2025.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pandas-market-calendars<6,>=5->finrl==0.3.8) (2.9.0.post0)\n",
      "Requirement already satisfied: exchange-calendars>=3.3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pandas-market-calendars<6,>=5->finrl==0.3.8) (4.11.1)\n",
      "Requirement already satisfied: ipython>=3.2.3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (8.25.0)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.10.6)\n",
      "Requirement already satisfied: pytz>=2014.10 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2024.1)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.16.2)\n",
      "Requirement already satisfied: seaborn>=0.7.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
      "Requirement already satisfied: empyrical-reloaded>=0.5.9 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.5.12)\n",
      "Requirement already satisfied: cvxpy>=1.1.19 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.7.2)\n",
      "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (2.0.14)\n",
      "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.1.7)\n",
      "Requirement already satisfied: filelock in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.19.1)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.19.2)\n",
      "Requirement already satisfied: protobuf>=3.20.3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (6.32.1)\n",
      "Requirement already satisfied: aiohttp_cors in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.8.1)\n",
      "Requirement already satisfied: colorful in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.5.7)\n",
      "Requirement already satisfied: py-spy>=0.4.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.1)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.75.0)\n",
      "Requirement already satisfied: opencensus in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.11.4)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.30.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-prometheus in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-proto in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.37.0)\n",
      "Requirement already satisfied: prometheus_client>=0.7.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.23.1)\n",
      "Requirement already satisfied: smart_open in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (7.3.1)\n",
      "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (20.34.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.6.4)\n",
      "Requirement already satisfied: pyarrow>=9.0.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (21.0.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2025.9.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.6.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from selenium<5,>=4->finrl==0.3.8) (0.31.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from selenium<5,>=4->finrl==0.3.8) (0.12.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from selenium<5,>=4->finrl==0.3.8) (4.15.0)\n",
      "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.2.0)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.8.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.12.0.88)\n",
      "Requirement already satisfied: pygame in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.20.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (5.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.66.4)\n",
      "Requirement already satisfied: rich in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (13.7.1)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.11.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (10.4.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jason\\miniconda3\\lib\\site-packages (from webdriver-manager<5,>=4->finrl==0.3.8) (1.1.0)\n",
      "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from wrds<4,>=3->finrl==0.3.8) (2.9.10)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.10.0)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.2)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.17.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.12.3)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.13.0)\n",
      "Requirement already satisfied: th in c:\\users\\jason\\miniconda3\\lib\\site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.4.1)\n",
      "Requirement already satisfied: pycares>=4.9.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8) (4.11.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (0.3.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\jason\\miniconda3\\lib\\site-packages (from click>=7.0->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (1.16.0)\n",
      "Requirement already satisfied: osqp>=0.6.2 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.4)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.11.1)\n",
      "Requirement already satisfied: scs>=3.2.4.post1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.8)\n",
      "Requirement already satisfied: bottleneck>=1.3.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from empyrical-reloaded>=0.5.9->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.6.0)\n",
      "Requirement already satisfied: pyluach in c:\\users\\jason\\miniconda3\\lib\\site-packages (from exchange-calendars>=3.3->pandas-market-calendars<6,>=5->finrl==0.3.8) (2.3.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\jason\\miniconda3\\lib\\site-packages (from exchange-calendars>=3.3->pandas-market-calendars<6,>=5->finrl==0.3.8) (1.0.0)\n",
      "Requirement already satisfied: korean_lunar_calendar in c:\\users\\jason\\miniconda3\\lib\\site-packages (from exchange-calendars>=3.3->pandas-market-calendars<6,>=5->finrl==0.3.8) (0.3.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.0.4)\n",
      "Requirement already satisfied: decorator in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.14.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.2.4)\n",
      "Requirement already satisfied: opentelemetry-api==1.37.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from opentelemetry-sdk>=1.30.0->ray[default,tune]<3,>=2->finrl==0.3.8) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from opentelemetry-sdk>=1.30.0->ray[default,tune]<3,>=2->finrl==0.3.8) (0.58b0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from opentelemetry-api==1.37.0->opentelemetry-sdk>=1.30.0->ray[default,tune]<3,>=2->finrl==0.3.8) (7.0.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (9.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.7)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.8) (3.2.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.3.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.3)\n",
      "Requirement already satisfied: ply<4.0,>=3.4 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from thriftpy2<=0.4.20,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\jason\\miniconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.4)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\jason\\miniconda3\\lib\\site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\jason\\miniconda3\\lib\\site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.10.6)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.1.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.25.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.2.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\jason\\miniconda3\\lib\\site-packages (from smart_open->ray[default,tune]<3,>=2->finrl==0.3.8) (1.14.1)\n",
      "Requirement already satisfied: niltype<2.0,>=0.3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.0.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jason\\miniconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.21)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.70.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.40.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jason\\miniconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.3)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.16.0)\n",
      "Requirement already satisfied: executing in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\jason\\miniconda3\\lib\\site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api==1.37.0->opentelemetry-sdk>=1.30.0->ray[default,tune]<3,>=2->finrl==0.3.8) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\jason\\miniconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git 'C:\\Users\\Jason\\AppData\\Local\\Temp\\pip-req-build-eel5o1w6'\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git 'C:\\Users\\Jason\\AppData\\Local\\Temp\\pip-install-r0ekff0k\\elegantrl_1f58174a581b4034a6a9b83445b87222'\n"
     ]
    }
   ],
   "source": [
    "## install finrl library\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xt1317y2ixSS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "\n",
    "import sys, pathlib\n",
    "sys.path.insert(0, str(pathlib.Path.cwd().parents[1]))\n",
    "from finai_contest.env_stock_trading.env_stock_trading_meta import StockTradingEnv_FinRLMeta\n",
    "from finai_contest.env_stock_trading.env_stock_trading_gym_anytrading import StockTradingEnv_gym_anytrading\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mFCP1YEhi6oi"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.724325</td>\n",
       "      <td>2.733032</td>\n",
       "      <td>2.556513</td>\n",
       "      <td>2.578127</td>\n",
       "      <td>7.460152e+08</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.944416</td>\n",
       "      <td>2.619211</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.724325</td>\n",
       "      <td>2.724325</td>\n",
       "      <td>39.189999</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.839302</td>\n",
       "      <td>2.887335</td>\n",
       "      <td>2.783164</td>\n",
       "      <td>2.796974</td>\n",
       "      <td>1.181608e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>2.944416</td>\n",
       "      <td>2.619211</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.781814</td>\n",
       "      <td>2.781814</td>\n",
       "      <td>39.080002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.792471</td>\n",
       "      <td>2.917054</td>\n",
       "      <td>2.773558</td>\n",
       "      <td>2.880430</td>\n",
       "      <td>1.289310e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>2.901000</td>\n",
       "      <td>2.669732</td>\n",
       "      <td>70.355312</td>\n",
       "      <td>45.847437</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.785366</td>\n",
       "      <td>2.785366</td>\n",
       "      <td>38.560001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.732131</td>\n",
       "      <td>2.776861</td>\n",
       "      <td>2.709616</td>\n",
       "      <td>2.756147</td>\n",
       "      <td>7.530488e+08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>2.880446</td>\n",
       "      <td>2.663668</td>\n",
       "      <td>50.429352</td>\n",
       "      <td>-30.767106</td>\n",
       "      <td>43.608273</td>\n",
       "      <td>2.772057</td>\n",
       "      <td>2.772057</td>\n",
       "      <td>43.389999</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.782865</td>\n",
       "      <td>2.796374</td>\n",
       "      <td>2.703011</td>\n",
       "      <td>2.714719</td>\n",
       "      <td>6.735008e+08</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>2.868583</td>\n",
       "      <td>2.679855</td>\n",
       "      <td>60.227087</td>\n",
       "      <td>-8.239411</td>\n",
       "      <td>48.358311</td>\n",
       "      <td>2.774219</td>\n",
       "      <td>2.774219</td>\n",
       "      <td>42.560001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3268</th>\n",
       "      <td>2021-12-27</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>176.796036</td>\n",
       "      <td>176.884268</td>\n",
       "      <td>173.599928</td>\n",
       "      <td>173.619525</td>\n",
       "      <td>7.491960e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.966004</td>\n",
       "      <td>179.988873</td>\n",
       "      <td>156.216783</td>\n",
       "      <td>65.037365</td>\n",
       "      <td>111.207256</td>\n",
       "      <td>48.878374</td>\n",
       "      <td>163.182837</td>\n",
       "      <td>153.508200</td>\n",
       "      <td>17.680000</td>\n",
       "      <td>9.832529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269</th>\n",
       "      <td>2021-12-28</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>175.776428</td>\n",
       "      <td>177.776459</td>\n",
       "      <td>175.031327</td>\n",
       "      <td>176.629389</td>\n",
       "      <td>7.914430e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.015728</td>\n",
       "      <td>180.195296</td>\n",
       "      <td>157.878025</td>\n",
       "      <td>64.013434</td>\n",
       "      <td>108.220225</td>\n",
       "      <td>50.226012</td>\n",
       "      <td>164.140365</td>\n",
       "      <td>154.110296</td>\n",
       "      <td>17.540001</td>\n",
       "      <td>8.468392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>175.864670</td>\n",
       "      <td>177.090173</td>\n",
       "      <td>174.648965</td>\n",
       "      <td>175.815647</td>\n",
       "      <td>6.234890e+07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.004565</td>\n",
       "      <td>180.775805</td>\n",
       "      <td>158.677922</td>\n",
       "      <td>64.064086</td>\n",
       "      <td>99.806902</td>\n",
       "      <td>48.433418</td>\n",
       "      <td>165.100506</td>\n",
       "      <td>154.771132</td>\n",
       "      <td>16.950001</td>\n",
       "      <td>6.345802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>174.707794</td>\n",
       "      <td>177.031359</td>\n",
       "      <td>174.599949</td>\n",
       "      <td>175.952910</td>\n",
       "      <td>5.977300e+07</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.846501</td>\n",
       "      <td>180.938414</td>\n",
       "      <td>159.831995</td>\n",
       "      <td>62.864038</td>\n",
       "      <td>91.887903</td>\n",
       "      <td>48.191102</td>\n",
       "      <td>165.989404</td>\n",
       "      <td>155.380545</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>7.366753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>174.090149</td>\n",
       "      <td>175.717606</td>\n",
       "      <td>173.786211</td>\n",
       "      <td>174.599948</td>\n",
       "      <td>6.406230e+07</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.618160</td>\n",
       "      <td>180.652223</td>\n",
       "      <td>161.472125</td>\n",
       "      <td>62.220326</td>\n",
       "      <td>79.373358</td>\n",
       "      <td>44.018730</td>\n",
       "      <td>166.776341</td>\n",
       "      <td>155.965142</td>\n",
       "      <td>17.219999</td>\n",
       "      <td>12.130258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3273 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date   tic       close        high         low        open  \\\n",
       "                                                                         \n",
       "0     2009-01-02  AAPL    2.724325    2.733032    2.556513    2.578127   \n",
       "1     2009-01-05  AAPL    2.839302    2.887335    2.783164    2.796974   \n",
       "2     2009-01-06  AAPL    2.792471    2.917054    2.773558    2.880430   \n",
       "3     2009-01-07  AAPL    2.732131    2.776861    2.709616    2.756147   \n",
       "4     2009-01-08  AAPL    2.782865    2.796374    2.703011    2.714719   \n",
       "...          ...   ...         ...         ...         ...         ...   \n",
       "3268  2021-12-27  AAPL  176.796036  176.884268  173.599928  173.619525   \n",
       "3269  2021-12-28  AAPL  175.776428  177.776459  175.031327  176.629389   \n",
       "3270  2021-12-29  AAPL  175.864670  177.090173  174.648965  175.815647   \n",
       "3271  2021-12-30  AAPL  174.707794  177.031359  174.599949  175.952910   \n",
       "3272  2021-12-31  AAPL  174.090149  175.717606  173.786211  174.599948   \n",
       "\n",
       "            volume  day      macd     boll_ub     boll_lb      rsi_30  \\\n",
       "                                                                        \n",
       "0     7.460152e+08  4.0  0.000000    2.944416    2.619211  100.000000   \n",
       "1     1.181608e+09  0.0  0.002580    2.944416    2.619211  100.000000   \n",
       "2     1.289310e+09  1.0  0.001835    2.901000    2.669732   70.355312   \n",
       "3     7.530488e+08  2.0 -0.000728    2.880446    2.663668   50.429352   \n",
       "4     6.735008e+08  3.0 -0.000086    2.868583    2.679855   60.227087   \n",
       "...            ...  ...       ...         ...         ...         ...   \n",
       "3268  7.491960e+07  0.0  4.966004  179.988873  156.216783   65.037365   \n",
       "3269  7.914430e+07  1.0  5.015728  180.195296  157.878025   64.013434   \n",
       "3270  6.234890e+07  2.0  5.004565  180.775805  158.677922   64.064086   \n",
       "3271  5.977300e+07  3.0  4.846501  180.938414  159.831995   62.864038   \n",
       "3272  6.406230e+07  4.0  4.618160  180.652223  161.472125   62.220326   \n",
       "\n",
       "          cci_30       dx_30  close_30_sma  close_60_sma        vix  \\\n",
       "                                                                      \n",
       "0      66.666667  100.000000      2.724325      2.724325  39.189999   \n",
       "1      66.666667  100.000000      2.781814      2.781814  39.080002   \n",
       "2      45.847437  100.000000      2.785366      2.785366  38.560001   \n",
       "3     -30.767106   43.608273      2.772057      2.772057  43.389999   \n",
       "4      -8.239411   48.358311      2.774219      2.774219  42.560001   \n",
       "...          ...         ...           ...           ...        ...   \n",
       "3268  111.207256   48.878374    163.182837    153.508200  17.680000   \n",
       "3269  108.220225   50.226012    164.140365    154.110296  17.540001   \n",
       "3270   99.806902   48.433418    165.100506    154.771132  16.950001   \n",
       "3271   91.887903   48.191102    165.989404    155.380545  17.330000   \n",
       "3272   79.373358   44.018730    166.776341    155.965142  17.219999   \n",
       "\n",
       "      turbulence  \n",
       "                  \n",
       "0       0.000000  \n",
       "1       0.000000  \n",
       "2       0.000000  \n",
       "3       0.000000  \n",
       "4       0.000000  \n",
       "...          ...  \n",
       "3268    9.832529  \n",
       "3269    8.468392  \n",
       "3270    6.345802  \n",
       "3271    7.366753  \n",
       "3272   12.130258  \n",
       "\n",
       "[3273 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./data/train_data.csv')\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']\n",
    "train = train[train[\"tic\"] == \"AAPL\"]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 1, State Space: 11\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WsOLoeNcJF8Q"
   },
   "outputs": [],
   "source": [
    "env_used = \"finrl\"\n",
    "# env_used = \"gym_anytrading\"\n",
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "\n",
    "if env_used ==\"finrl\":\n",
    "    env_kwargs = {\n",
    "        \"hmax\": 100,\n",
    "        \"initial_amount\": 1000000,\n",
    "        \"num_stock_shares\": num_stock_shares,\n",
    "        \"buy_cost_pct\": buy_cost_list,\n",
    "        \"sell_cost_pct\": sell_cost_list,\n",
    "        \"state_space\": state_space,\n",
    "        \"stock_dim\": stock_dimension,\n",
    "        \"tech_indicator_list\": INDICATORS,\n",
    "        \"action_space\": stock_dimension,\n",
    "        \"reward_scaling\": 1e-4\n",
    "    }\n",
    "\n",
    "\n",
    "    e_train_gym = StockTradingEnv_FinRLMeta(df = train, **env_kwargs)\n",
    "\n",
    "elif env_used ==\"gym_anytrading\":\n",
    "    env_kwargs = {\n",
    "    \"hmax\": np.inf,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": 2*stock_dimension,\n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"window_size\": 30\n",
    "    }\n",
    "\n",
    "    e_train_gym = StockTradingEnv_gym_anytrading(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
   },
   "outputs": [],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_aapl = train[train[\"tic\"] == \"aapl\"]\n",
    "# train_gym_anytrade = pd.DataFrame()\n",
    "# train_gym_anytrade['Time'] = pd.to_datetime(train_aapl['date'])  # Convert to datetime\n",
    "# train_gym_anytrade['Open'] = train_aapl['open']\n",
    "# train_gym_anytrade['High'] = train_aapl['high']\n",
    "# train_gym_anytrade['Low'] = train_aapl['low']\n",
    "# train_gym_anytrade['Close'] = train_aapl['close']\n",
    "# train_gym_anytrade['Volume'] = train_aapl['volume']\n",
    "# train_aapl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "# import gym_anytrading\n",
    "# env_train = gym.make(\n",
    "#     'stocks-v0',\n",
    "#     df=train_gym_anytrade,\n",
    "#     window_size=30,\n",
    "#     frame_bound=(30, len(train_gym_anytrade))\n",
    "# )\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# def get_sb_env(self):\n",
    "#     e = DummyVecEnv([lambda: self])\n",
    "#     obs = e.reset()\n",
    "#     return e, obs\n",
    "# # Patch the method\n",
    "# env_train = env_train.env\n",
    "# env_train = env_train.env\n",
    "# env_train.get_sb_env = get_sb_env.__get__(env_train)\n",
    "# env_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = False\n",
    "if_using_ddpg = False\n",
    "if_using_ppo = True\n",
    "if_using_td3 = False\n",
    "if_using_sac = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
   },
   "outputs": [],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "# if if_using_ddpg:\n",
    "#   # set up logger\n",
    "#   tmp_path = RESULTS_DIR + '/ddpg'\n",
    "#   new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#   # Set new logger\n",
    "#   model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/              |               |\n",
      "|    fps             | 1086          |\n",
      "|    iterations      | 1             |\n",
      "|    time_elapsed    | 1             |\n",
      "|    total_timesteps | 2048          |\n",
      "| train/             |               |\n",
      "|    learning_rate   | 0.00025       |\n",
      "|    reward          | 0.0           |\n",
      "|    reward_max      | 0.060199596   |\n",
      "|    reward_mean     | 0.00015746405 |\n",
      "|    reward_min      | -0.06868129   |\n",
      "--------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 556           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 7             |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0053390255  |\n",
      "|    clip_fraction        | 0.0386        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0175       |\n",
      "|    n_updates            | 69            |\n",
      "|    policy_gradient_loss | -0.00195      |\n",
      "|    reward               | -0.0058539496 |\n",
      "|    reward_max           | 2.315391      |\n",
      "|    reward_mean          | 0.012020233   |\n",
      "|    reward_min           | -2.267081     |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 0.0006        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 435          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041039595 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.251        |\n",
      "|    n_updates            | 79           |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    reward               | 1.4966215    |\n",
      "|    reward_max           | 10.82477     |\n",
      "|    reward_mean          | 0.032424964  |\n",
      "|    reward_min           | -13.035      |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 0.505        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 400          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046283184 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.11         |\n",
      "|    n_updates            | 89           |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    reward               | 1.0392851    |\n",
      "|    reward_max           | 15.886767    |\n",
      "|    reward_mean          | 0.09957202   |\n",
      "|    reward_min           | -17.305271   |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 6.58         |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 380            |\n",
      "|    iterations           | 5              |\n",
      "|    time_elapsed         | 26             |\n",
      "|    total_timesteps      | 10240          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000115611125 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.41          |\n",
      "|    explained_variance   | -0.000533      |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 15             |\n",
      "|    n_updates            | 99             |\n",
      "|    policy_gradient_loss | -0.000197      |\n",
      "|    reward               | 0.13267894     |\n",
      "|    reward_max           | 35.82318       |\n",
      "|    reward_mean          | 0.247501       |\n",
      "|    reward_min           | -37.095        |\n",
      "|    std                  | 0.992          |\n",
      "|    value_loss           | 26.5           |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 372          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006396872 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.098        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 80.5         |\n",
      "|    n_updates            | 109          |\n",
      "|    policy_gradient_loss | -0.00026     |\n",
      "|    reward               | 1.4694432    |\n",
      "|    reward_max           | 7.707467     |\n",
      "|    reward_mean          | 0.04928405   |\n",
      "|    reward_min           | -7.359019    |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 153          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 354          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032280888 |\n",
      "|    clip_fraction        | 0.00635      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.04         |\n",
      "|    n_updates            | 119          |\n",
      "|    policy_gradient_loss | 1.7e-05      |\n",
      "|    reward               | -0.09170261  |\n",
      "|    reward_max           | 33.06741     |\n",
      "|    reward_mean          | 0.20582582   |\n",
      "|    reward_min           | -35.36373    |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 6.27         |\n",
      "------------------------------------------\n",
      "day: 3272, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4518016.35\n",
      "total_reward: 3518016.35\n",
      "total_cost: 7590.92\n",
      "total_trades: 3250\n",
      "Sharpe: 0.853\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 339          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.000561928  |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.176        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 44.6         |\n",
      "|    n_updates            | 129          |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    reward               | -0.003989546 |\n",
      "|    reward_max           | 23.123205    |\n",
      "|    reward_mean          | 0.16873977   |\n",
      "|    reward_min           | -25.103952   |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 130          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 332         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004826011 |\n",
      "|    clip_fraction        | 0.0134      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.288       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 31.1        |\n",
      "|    n_updates            | 139         |\n",
      "|    policy_gradient_loss | -0.00135    |\n",
      "|    reward               | -1.2409095  |\n",
      "|    reward_max           | 5.6420054   |\n",
      "|    reward_mean          | 0.027063804 |\n",
      "|    reward_min           | -4.2855797  |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 65.5        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 327           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 62            |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021376627 |\n",
      "|    clip_fraction        | 0.0041        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.598         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.01          |\n",
      "|    n_updates            | 149           |\n",
      "|    policy_gradient_loss | 0.000577      |\n",
      "|    reward               | 0.14477712    |\n",
      "|    reward_max           | 42.08907      |\n",
      "|    reward_mean          | 0.29512945    |\n",
      "|    reward_min           | -43.768818    |\n",
      "|    std                  | 0.995         |\n",
      "|    value_loss           | 5.36          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 325          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 69           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013591989 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.25         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 111          |\n",
      "|    n_updates            | 159          |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    reward               | -12.921185   |\n",
      "|    reward_max           | 34.31625     |\n",
      "|    reward_mean          | 0.14531673   |\n",
      "|    reward_min           | -41.262768   |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 212          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 323          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014616098 |\n",
      "|    clip_fraction        | 0.00259      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.363        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 48.1         |\n",
      "|    n_updates            | 169          |\n",
      "|    policy_gradient_loss | -0.000433    |\n",
      "|    reward               | 0.5244798    |\n",
      "|    reward_max           | 46.53672     |\n",
      "|    reward_mean          | 0.2192826    |\n",
      "|    reward_min           | -48.647614   |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 88.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 323          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 82           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.001304154  |\n",
      "|    clip_fraction        | 0.00659      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.388        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 48           |\n",
      "|    n_updates            | 179          |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    reward               | -0.028025737 |\n",
      "|    reward_max           | 44.422318    |\n",
      "|    reward_mean          | 0.3198806    |\n",
      "|    reward_min           | -46.483566   |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 171          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 317           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 90            |\n",
      "|    total_timesteps      | 28672         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00055669073 |\n",
      "|    clip_fraction        | 0.000146      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.344         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 92.6          |\n",
      "|    n_updates            | 189           |\n",
      "|    policy_gradient_loss | -0.000982     |\n",
      "|    reward               | -7.094693     |\n",
      "|    reward_max           | 10.214878     |\n",
      "|    reward_mean          | 0.047408324   |\n",
      "|    reward_min           | -13.519531    |\n",
      "|    std                  | 0.993         |\n",
      "|    value_loss           | 235           |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 315         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 97          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005074747 |\n",
      "|    clip_fraction        | 0.0181      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.1        |\n",
      "|    n_updates            | 199         |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    reward               | -0.5974604  |\n",
      "|    reward_max           | 43.524143   |\n",
      "|    reward_mean          | 0.28686666  |\n",
      "|    reward_min           | -45.650448  |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 14.3        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 315           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 103           |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.001438108   |\n",
      "|    clip_fraction        | 0.00415       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.479         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 106           |\n",
      "|    n_updates            | 209           |\n",
      "|    policy_gradient_loss | -0.00263      |\n",
      "|    reward               | 0.00028164027 |\n",
      "|    reward_max           | 49.51351      |\n",
      "|    reward_mean          | 0.37231335    |\n",
      "|    reward_min           | -51.59646     |\n",
      "|    std                  | 0.993         |\n",
      "|    value_loss           | 204           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 314           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 110           |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00058369245 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.389         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 138           |\n",
      "|    n_updates            | 219           |\n",
      "|    policy_gradient_loss | -0.000388     |\n",
      "|    reward               | -0.66350245   |\n",
      "|    reward_max           | 7.4392605     |\n",
      "|    reward_mean          | 0.03511816    |\n",
      "|    reward_min           | -5.717238     |\n",
      "|    std                  | 0.992         |\n",
      "|    value_loss           | 298           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 310          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 118          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052784123 |\n",
      "|    clip_fraction        | 0.0361       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.753        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.54         |\n",
      "|    n_updates            | 229          |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    reward               | 0.26631933   |\n",
      "|    reward_max           | 50.397522    |\n",
      "|    reward_mean          | 0.36370566   |\n",
      "|    reward_min           | -52.289192   |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 7.72         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 309         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 125         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003558569 |\n",
      "|    clip_fraction        | 0.0169      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 139         |\n",
      "|    n_updates            | 239         |\n",
      "|    policy_gradient_loss | -0.00238    |\n",
      "|    reward               | -1.4056689  |\n",
      "|    reward_max           | 45.19889    |\n",
      "|    reward_mean          | 0.2167382   |\n",
      "|    reward_min           | -54.28514   |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 304         |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 306           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 133           |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00051227864 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.529         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 97.7          |\n",
      "|    n_updates            | 249           |\n",
      "|    policy_gradient_loss | -0.000279     |\n",
      "|    reward               | 2.0307698     |\n",
      "|    reward_max           | 61.53162      |\n",
      "|    reward_mean          | 0.2858036     |\n",
      "|    reward_min           | -64.39166     |\n",
      "|    std                  | 0.989         |\n",
      "|    value_loss           | 171           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 305          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 140          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.648682e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.48         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 131          |\n",
      "|    n_updates            | 259          |\n",
      "|    policy_gradient_loss | -0.000205    |\n",
      "|    reward               | -0.1123647   |\n",
      "|    reward_max           | 61.417324    |\n",
      "|    reward_mean          | 0.45620146   |\n",
      "|    reward_min           | -63.963272   |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 268          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 304           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 148           |\n",
      "|    total_timesteps      | 45056         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054597564 |\n",
      "|    clip_fraction        | 0.00293       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.368         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 143           |\n",
      "|    n_updates            | 269           |\n",
      "|    policy_gradient_loss | -0.000458     |\n",
      "|    reward               | 3.273885      |\n",
      "|    reward_max           | 17.131517     |\n",
      "|    reward_mean          | 0.073446505   |\n",
      "|    reward_min           | -22.62303     |\n",
      "|    std                  | 0.988         |\n",
      "|    value_loss           | 463           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 302          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 155          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030594226 |\n",
      "|    clip_fraction        | 0.00239      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.785        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 31           |\n",
      "|    n_updates            | 279          |\n",
      "|    policy_gradient_loss | -0.000802    |\n",
      "|    reward               | 0.36686546   |\n",
      "|    reward_max           | 63.242493    |\n",
      "|    reward_mean          | 0.43177533   |\n",
      "|    reward_min           | -66.163185   |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 55           |\n",
      "------------------------------------------\n",
      "day: 3272, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10768295.44\n",
      "total_reward: 9768295.44\n",
      "total_cost: 5497.93\n",
      "total_trades: 3247\n",
      "Sharpe: 0.959\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 301           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 162           |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017741125 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.518         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 274           |\n",
      "|    n_updates            | 289           |\n",
      "|    policy_gradient_loss | 4.43e-05      |\n",
      "|    reward               | 0.012754955   |\n",
      "|    reward_max           | 60.50018      |\n",
      "|    reward_mean          | 0.4658694     |\n",
      "|    reward_min           | -63.294315    |\n",
      "|    std                  | 0.985         |\n",
      "|    value_loss           | 428           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 302          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 169          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005404232 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.403        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 124          |\n",
      "|    n_updates            | 299          |\n",
      "|    policy_gradient_loss | -0.000642    |\n",
      "|    reward               | -0.4930099   |\n",
      "|    reward_max           | 8.81402      |\n",
      "|    reward_mean          | 0.051149603  |\n",
      "|    reward_min           | -7.1720114   |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 449          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 303          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 175          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007534175 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.501        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.52         |\n",
      "|    n_updates            | 309          |\n",
      "|    policy_gradient_loss | -0.00189     |\n",
      "|    reward               | 1.0913388    |\n",
      "|    reward_max           | 54.963097    |\n",
      "|    reward_mean          | 0.3855877    |\n",
      "|    reward_min           | -57.49532    |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 9.9          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 303          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 182          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016339511 |\n",
      "|    clip_fraction        | 0.00347      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.564        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 165          |\n",
      "|    n_updates            | 319          |\n",
      "|    policy_gradient_loss | -0.000787    |\n",
      "|    reward               | 14.787048    |\n",
      "|    reward_max           | 57.870895    |\n",
      "|    reward_mean          | 0.27389756   |\n",
      "|    reward_min           | -50.846386   |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 357          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 303          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 189          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022208982 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.655        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 47.1         |\n",
      "|    n_updates            | 329          |\n",
      "|    policy_gradient_loss | -0.00353     |\n",
      "|    reward               | -1.3744649   |\n",
      "|    reward_max           | 42.67234     |\n",
      "|    reward_mean          | 0.19369504   |\n",
      "|    reward_min           | -60.57036    |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 211          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 302          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 196          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019679754 |\n",
      "|    clip_fraction        | 0.000879     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.628        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 104          |\n",
      "|    n_updates            | 339          |\n",
      "|    policy_gradient_loss | -8.92e-05    |\n",
      "|    reward               | 0.17430837   |\n",
      "|    reward_max           | 57.321903    |\n",
      "|    reward_mean          | 0.42485178   |\n",
      "|    reward_min           | -59.94309    |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 202          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 302          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 202          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006488244 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.566        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 204          |\n",
      "|    n_updates            | 349          |\n",
      "|    policy_gradient_loss | -0.000173    |\n",
      "|    reward               | 2.2581       |\n",
      "|    reward_max           | 13.438702    |\n",
      "|    reward_mean          | 0.04108406   |\n",
      "|    reward_min           | -18.956034   |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 395          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 302          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 210          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038252217 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.777        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.64         |\n",
      "|    n_updates            | 359          |\n",
      "|    policy_gradient_loss | -0.003       |\n",
      "|    reward               | -0.19943208  |\n",
      "|    reward_max           | 49.675243    |\n",
      "|    reward_mean          | 0.34764814   |\n",
      "|    reward_min           | -51.869152   |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 38.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 301          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 217          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022498618 |\n",
      "|    clip_fraction        | 0.00161      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.692        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 104          |\n",
      "|    n_updates            | 369          |\n",
      "|    policy_gradient_loss | -0.000471    |\n",
      "|    reward               | 0.029706549  |\n",
      "|    reward_max           | 58.65566     |\n",
      "|    reward_mean          | 0.4498524    |\n",
      "|    reward_min           | -61.426617   |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 249          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 302          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 223          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032814764 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.53         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 236          |\n",
      "|    n_updates            | 379          |\n",
      "|    policy_gradient_loss | -0.000201    |\n",
      "|    reward               | -4.468593    |\n",
      "|    reward_max           | 9.061081     |\n",
      "|    reward_mean          | 0.048553813  |\n",
      "|    reward_min           | -7.557316    |\n",
      "|    std                  | 0.969        |\n",
      "|    value_loss           | 416          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 303           |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 229           |\n",
      "|    total_timesteps      | 69632         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018465321 |\n",
      "|    clip_fraction        | 0.00747       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0.722         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.09          |\n",
      "|    n_updates            | 389           |\n",
      "|    policy_gradient_loss | 0.000217      |\n",
      "|    reward               | 0.81495136    |\n",
      "|    reward_max           | 57.00076      |\n",
      "|    reward_mean          | 0.40661904    |\n",
      "|    reward_min           | -59.628834    |\n",
      "|    std                  | 0.973         |\n",
      "|    value_loss           | 11.2          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 303          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 235          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031560604 |\n",
      "|    clip_fraction        | 0.00771      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.613        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 298          |\n",
      "|    n_updates            | 399          |\n",
      "|    policy_gradient_loss | -0.000218    |\n",
      "|    reward               | -10.569313   |\n",
      "|    reward_max           | 59.38025     |\n",
      "|    reward_mean          | 0.25849655   |\n",
      "|    reward_min           | -62.08974    |\n",
      "|    std                  | 0.969        |\n",
      "|    value_loss           | 381          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 304          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 242          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012829828 |\n",
      "|    clip_fraction        | 0.0135       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.677        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 200          |\n",
      "|    n_updates            | 409          |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    reward               | -0.62730175  |\n",
      "|    reward_max           | 43.8985      |\n",
      "|    reward_mean          | 0.22648093   |\n",
      "|    reward_min           | -41.72463    |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 253          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 304          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009200977 |\n",
      "|    clip_fraction        | 0.00425      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.695        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 89.8         |\n",
      "|    n_updates            | 419          |\n",
      "|    policy_gradient_loss | -0.000925    |\n",
      "|    reward               | 0.041864924  |\n",
      "|    reward_max           | 58.99931     |\n",
      "|    reward_mean          | 0.4337021    |\n",
      "|    reward_min           | -61.728184   |\n",
      "|    std                  | 0.969        |\n",
      "|    value_loss           | 185          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 305          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 254          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023825718 |\n",
      "|    clip_fraction        | 0.0199       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.561        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 174          |\n",
      "|    n_updates            | 429          |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    reward               | -1.0840244   |\n",
      "|    reward_max           | 16.922068    |\n",
      "|    reward_mean          | 0.07544109   |\n",
      "|    reward_min           | -23.917456   |\n",
      "|    std                  | 0.973        |\n",
      "|    value_loss           | 420          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 306          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 260          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013541412 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 22.8         |\n",
      "|    n_updates            | 439          |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    reward               | 0.11502578   |\n",
      "|    reward_max           | 62.392723    |\n",
      "|    reward_mean          | 0.4254423    |\n",
      "|    reward_min           | -65.290596   |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 69.2         |\n",
      "------------------------------------------\n",
      "day: 3272, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10820368.54\n",
      "total_reward: 9820368.54\n",
      "total_cost: 5945.59\n",
      "total_trades: 3248\n",
      "Sharpe: 0.961\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 306          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 267          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011885987 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.656        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 183          |\n",
      "|    n_updates            | 449          |\n",
      "|    policy_gradient_loss | -0.000483    |\n",
      "|    reward               | -0.0109013   |\n",
      "|    reward_max           | 60.881508    |\n",
      "|    reward_mean          | 0.46575087   |\n",
      "|    reward_min           | -63.699364   |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 403          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 306          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 274          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025932724 |\n",
      "|    clip_fraction        | 0.0216       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.554        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 178          |\n",
      "|    n_updates            | 459          |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    reward               | 1.0194676    |\n",
      "|    reward_max           | 7.877273     |\n",
      "|    reward_mean          | 0.04085662   |\n",
      "|    reward_min           | -6.849825    |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 450          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 306         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 280         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001516011 |\n",
      "|    clip_fraction        | 0.0164      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.08        |\n",
      "|    n_updates            | 469         |\n",
      "|    policy_gradient_loss | -0.000967   |\n",
      "|    reward               | -0.37675276 |\n",
      "|    reward_max           | 52.557796   |\n",
      "|    reward_mean          | 0.37631786  |\n",
      "|    reward_min           | -54.983925  |\n",
      "|    std                  | 0.977       |\n",
      "|    value_loss           | 8.21        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 306          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 287          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020098453 |\n",
      "|    clip_fraction        | 0.0142       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.69         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 170          |\n",
      "|    n_updates            | 479          |\n",
      "|    policy_gradient_loss | -0.000948    |\n",
      "|    reward               | 0.49167645   |\n",
      "|    reward_max           | 55.15796     |\n",
      "|    reward_mean          | 0.26502836   |\n",
      "|    reward_min           | -57.7328     |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 319          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 306          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 294          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025937553 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.741        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 140          |\n",
      "|    n_updates            | 489          |\n",
      "|    policy_gradient_loss | -0.000464    |\n",
      "|    reward               | -1.2356064   |\n",
      "|    reward_max           | 35.96588     |\n",
      "|    reward_mean          | 0.18319933   |\n",
      "|    reward_min           | -38.928707   |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 222          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 306          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 301          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014781649 |\n",
      "|    clip_fraction        | 0.00913      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.73         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 62.3         |\n",
      "|    n_updates            | 499          |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    reward               | -0.33233783  |\n",
      "|    reward_max           | 59.75312     |\n",
      "|    reward_mean          | 0.4439422    |\n",
      "|    reward_min           | -62.84303    |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 157          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 305         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 308         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003164544 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.599       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 190         |\n",
      "|    n_updates            | 509         |\n",
      "|    policy_gradient_loss | -0.00103    |\n",
      "|    reward               | 1.1930757   |\n",
      "|    reward_max           | 16.65839    |\n",
      "|    reward_mean          | 0.0815192   |\n",
      "|    reward_min           | -23.430311  |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 436         |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 303           |\n",
      "|    iterations           | 47            |\n",
      "|    time_elapsed         | 316           |\n",
      "|    total_timesteps      | 96256         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048512808 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.872         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 38.5          |\n",
      "|    n_updates            | 519           |\n",
      "|    policy_gradient_loss | -0.000157     |\n",
      "|    reward               | -0.31128973   |\n",
      "|    reward_max           | 61.38933      |\n",
      "|    reward_mean          | 0.41200116    |\n",
      "|    reward_min           | -64.0509      |\n",
      "|    std                  | 0.989         |\n",
      "|    value_loss           | 69.7          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 304          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 323          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028672803 |\n",
      "|    clip_fraction        | 0.00874      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 177          |\n",
      "|    n_updates            | 529          |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    reward               | -0.006863341 |\n",
      "|    reward_max           | 61.26492     |\n",
      "|    reward_mean          | 0.46847326   |\n",
      "|    reward_min           | -64.42713    |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 383          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 303          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 330          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036716706 |\n",
      "|    clip_fraction        | 0.0164       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.48         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 213          |\n",
      "|    n_updates            | 539          |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    reward               | 1.0114611    |\n",
      "|    reward_max           | 8.828475     |\n",
      "|    reward_mean          | 0.052300613  |\n",
      "|    reward_min           | -7.36478     |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 491          |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=100000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo_\"+env_used) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# TD3_PARAMS = {\"batch_size\": 100, \n",
    "#               \"buffer_size\": 1000000, \n",
    "#               \"learning_rate\": 0.001}\n",
    "\n",
    "# model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "# if if_using_td3:\n",
    "#   # set up logger\n",
    "#   tmp_path = RESULTS_DIR + '/td3'\n",
    "#   new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#   # Set new logger\n",
    "#   model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# SAC_PARAMS = {\n",
    "#     \"batch_size\": 128,\n",
    "#     \"buffer_size\": 100000,\n",
    "#     \"learning_rate\": 0.0001,\n",
    "#     \"learning_starts\": 100,\n",
    "#     \"ent_coef\": \"auto_0.1\",\n",
    "# }\n",
    "\n",
    "# model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "# if if_using_sac:\n",
    "#   # set up logger\n",
    "#   tmp_path = RESULTS_DIR + '/sac'\n",
    "#   new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#   # Set new logger\n",
    "#   model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
